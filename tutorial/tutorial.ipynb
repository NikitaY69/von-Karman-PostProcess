{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f2baadc",
   "metadata": {},
   "source": [
    "# <center>How to use the von-Karman-PostProcess module ?</center>\n",
    "## <center>By Nikita Allaglo</center>\n",
    "### <center>16/07/2023</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf52ae9b",
   "metadata": {},
   "source": [
    "# 1. Building a database\n",
    "\n",
    "The SFEMaNS data I based all my work upon is located in the following directory on **Lab-IA**:\n",
    "\n",
    "<code>/mnt/beegfs/projects/sfemans_ai/data_hugues_Re_6000_anticontra</code>\n",
    "\n",
    "We thereafter show how to construct the database with the Database class. <br>\n",
    "<ins>This section's blocks should only be executed **once**.</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd79881f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vkpp import Database\n",
    "\n",
    "source = '/mnt/beegfs/projects/sfemans_ai/data_hugues_Re_6000_anticontra/'       # SFEMaNS full data\n",
    "sfx = 'tfin_374p8/'                                                              # eventual subdirs ('' default)\n",
    "target = '/mnt/beegfs/projects/sfemans_ai/connectors/'                           # name of connectors dir\n",
    "M = 255                                                                          # number of fourier modes\n",
    "dic = 'data_hugues.pkl'                                                          # name of the created dictionnary\n",
    "db = Database(source, sfx=sfx, target=target, M=M, dic=dic, space='phys')        # space 'phys' by default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac36989d",
   "metadata": {},
   "source": [
    "### If the source data has many sub dirs (sfx argument), complete the database as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7794bf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfxs = ['tfin_385p3', 'tfin_395p3', 'tfin_406p3']\n",
    "for sfx in sfxs:\n",
    "    print('######################################################################')\n",
    "    print(sfx)\n",
    "    db.make(sfx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d694bab5",
   "metadata": {},
   "source": [
    "# 2. Post-Processing bird's eye view\n",
    "The database yields the following data structure:\n",
    "$$(1,T,P,N),$$\n",
    "$T$ being the number of snapshots, $P$ the number of planes and $N$ the number of points per mesh (1 can finally be replaced with the number of intrinsic components). **You must make sure to perpetuate the same structure throughout your post-processing. Always slice with $\\texttt{A[:, [i], :, :]}$ instead of $\\texttt{A[:, i, :, :]}$.**\n",
    "\n",
    "## 2.1. Basic usage\n",
    "The following section provides simple examples on how to use the Stats class.<br>\n",
    "The present directory is provided with two submission scripts: \n",
    "- <code>jupyter_cpu.sh</code>; using 7 CPU-cores (14 threads) under 150 GB of CPU RAM.\n",
    "- <code>jupyter_gpu.sh</code>; using 1 GPU (node 1-5) + 7 CPU-cores (14 threads) under 48 GB of GDDR6 RAM.\n",
    "\n",
    "**The notebook automatically checks if you're using CPU or GPU ressources. The cell blocks are executable regardless of the ressources.**\n",
    "**If you properly installed the package, the correct version will automatically be imported.** In case you did not, make sure the branch you're on is coherent with the resources you were allocated. The following commands might be helpful for git:\n",
    "```console\n",
    "$ git branch -a\n",
    "```\n",
    "to check the available branches and \n",
    "```console\n",
    "$ git checkout BRANCH_NAME\n",
    "```\n",
    "to switch between branches.\n",
    "\n",
    "### 2.1.1. Setting the Post-Processing module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d70468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dask.array as da\n",
    "\n",
    "# Verifying the device\n",
    "try:\n",
    "    import cupy as cp\n",
    "    cp.array([0])\n",
    "    gpu=True\n",
    "    from vkcupp import Stats  # importing gpu version\n",
    "except:\n",
    "    gpu=False\n",
    "    from vkpp import Stats    # importing cpu version\n",
    "    \n",
    "# Setting the Stats object\n",
    "db_path = '/mnt/beegfs/projects/sfemans_ai/connectors/data_hugues.pkl'\n",
    "pp = Stats(db_path, 'full')   # 'full' specifies that no points are to be excluded in the statistics\n",
    "pp.dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b714c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pp.db\n",
    "list(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1a9196",
   "metadata": {},
   "source": [
    "### 2.1.2. Parallel-loading with Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06cc37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = data['omega']['field']\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa0515d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w[:, [0], :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d463bcc",
   "metadata": {},
   "source": [
    "As you can see, <code>w</code> is still delayed no matter the operation applied to it. Because of bad experience,  regardless of the branch, <ins>prior to any calculation</ins>, you should ALWAYS load the data on the RAM instead of piling delayed dask operations or converting from Dask to CuPy. Piling delayed operations can quickly give rise to memory problems or performance bottlenecks. <br>\n",
    "On the other hand, the loading can be made more stable by rechunking your data properly to the resources at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177976ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = w.rechunk((3, 1, pp.dims[2], pp.dims[3]//14))\n",
    "w_0 = w[:, [0], :, :].compute()\n",
    "\n",
    "if gpu:\n",
    "    # transferring to CUDA\n",
    "    w_0 = cp.array(w_0, copy=False)\n",
    "\n",
    "w_0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e18496a",
   "metadata": {},
   "source": [
    "### 2.1.3. Norm\n",
    "For any more-than-one-component field $\\boldsymbol{X}$ defined **anywhere**, returns $\\sqrt{\\sum_i X_i^2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7986d13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_norm = pp.norm(w_0)\n",
    "w_norm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa720953",
   "metadata": {},
   "source": [
    "### 2.1.4. Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8e251a",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_avg = pp.mean(w_norm, type='both')\n",
    "w_avg\n",
    "# since here there is no temporal axis, type='both' is the same as type='spatial'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b4af5e",
   "metadata": {},
   "source": [
    "### 2.1.5. PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53106aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 1000\n",
    "w_pdf, w_edges = pp.pdf(w_norm/w_avg, bins=bins)\n",
    "# you can specify a range with range=[field_min, field_max]\n",
    "# by default it computes the real min and max of the fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a9e210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if gpu:\n",
    "    plt.stairs(w_pdf.get(), w_edges.get()) # transferring back to the CPU\n",
    "else:\n",
    "    plt.stairs(w_pdf, w_edges)\n",
    "    \n",
    "# plotting params\n",
    "plt.xlabel(r'$\\frac{\\omega}{\\langle\\omega\\rangle}$')\n",
    "plt.ylabel(r'PDF')\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(0, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7146a66a",
   "metadata": {},
   "source": [
    "### 2.1.6. Joint PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff4c4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading second field\n",
    "Du = data['Du_2']['field'].rechunk((1, 1, pp.dims[2], pp.dims[3]//14))\n",
    "Du_0 = Du[:, [0], :, :].compute()\n",
    "\n",
    "if gpu:\n",
    "    # transferring to CUDA\n",
    "    Du_0 = cp.array(Du_0, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07e4653",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.045\n",
    "\n",
    "Du_w, [Du_edges, w_edges] = pp.joint_pdf(Du_0/epsilon, w_norm/w_avg, bins=bins, log=False, save='JOINT_DuVSw.dat')\n",
    "# log=True directly returns log10(2DHistogram)\n",
    "# you can specify the ranges with ranges=[[field1_min, field1_max], [field2_min, field2_max]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a9e9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "\n",
    "if gpu:\n",
    "    X, Y = np.meshgrid(Du_edges.get(), w_edges.get())\n",
    "    fp = plt.pcolormesh(X, Y, cp.log10(Du_w).get())\n",
    "else:\n",
    "    X, Y = np.meshgrid(Du_edges, w_edges)\n",
    "    fp = plt.pcolormesh(X, Y, np.log10(Du_w))\n",
    "    \n",
    "# plotting params\n",
    "plt.xlabel(r'$\\frac{\\mathscr{D}^u_{\\ell=26.5\\eta}}{\\epsilon}(t=0)$')\n",
    "plt.ylabel(r'$\\frac{\\omega}{\\langle\\omega\\rangle}(t=0)$')\n",
    "plt.set_cmap('gist_ncar')\n",
    "plt.colorbar(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ba1c24",
   "metadata": {},
   "source": [
    "If you reached this point **under the GPU version**, we suggest you to open a terminal on the node you're running (you can do so directly in jupyter: open back your jupyter file-browser and under *new*, select *Terminal*). Now type:\n",
    "```console\n",
    "$ nvidia-smi\n",
    "```\n",
    "You should now check your running Process in the Processes section. The memory usage should be close to 47-48 GB. The used GPU (nodes 1-5) have 48 GB of GDDR6 RAM. We now must manually free the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7fa657",
   "metadata": {},
   "outputs": [],
   "source": [
    "if gpu:\n",
    "    del w_0, w_norm, Du_w          \n",
    "    cp._default_memory_pool.free_all_blocks() # deleting the 'ghosts'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630cc2f0",
   "metadata": {},
   "source": [
    "### 2.1.7. Loading Joint PDF from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93ca71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if gpu:\n",
    "    Du_w = cp.fromfile('JOINT_DuVSw.dat')\n",
    "else:\n",
    "    Du_w = np.fromfile('JOINT_DuVSw.dat')\n",
    "\n",
    "Du_w = pp.load_reshaped(Du_w, bins)\n",
    "Du_w.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242f7fb8",
   "metadata": {},
   "source": [
    "### 2.1.8. Extract PDF from Joint PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc788e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges = [[Du_edges[0], Du_edges[-1]], [w_edges[0], w_edges[-1]]]\n",
    "\n",
    "Du_pdf = pp.extract_pdf(Du_w, 0, ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f91ba53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "\n",
    "if gpu:\n",
    "    plt.stairs(Du_pdf.get(), Du_edges.get()) # transferring back to the CPU\n",
    "else:\n",
    "    plt.stairs(Du_pdf, Du_edges)\n",
    "    \n",
    "# plotting params\n",
    "plt.xlabel(r'$\\frac{\\mathscr{D}^u_{\\ell=26.5\\eta}}{\\epsilon}$')\n",
    "plt.ylabel(r'PDF')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12975000",
   "metadata": {},
   "source": [
    "### 2.1.9. Conditional average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c2d401",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_given_Du = pp.conditional_avg(Du_w, 0, bins=bins, ranges=ranges)\n",
    "Du_given_w = pp.conditional_avg(Du_w, 1, bins=bins, ranges=ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b0389e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "%matplotlib inline\n",
    "# midpoint of each bin\n",
    "ech_w = pp.mid_points(w_edges)\n",
    "ech_Du = pp.mid_points(Du_edges)\n",
    "\n",
    "# plot labels\n",
    "xplot = r'$\\mathbb{E}\\left(\\frac{\\omega}{\\langle\\omega\\rangle}\\mid\\frac{\\mathscr{D}^u_{\\ell=26.5\\eta}}{\\epsilon}\\right)$'\n",
    "yplot = r'$\\mathbb{E}\\left(\\frac{\\mathscr{D}^u_{\\ell=26.5\\eta}}{\\epsilon}\\mid\\frac{\\omega}{\\langle\\omega\\rangle}\\right)$'\n",
    "\n",
    "if gpu:\n",
    "    # joint pdf\n",
    "    fp = plt.pcolormesh(X, Y, cp.log10(Du_w).get())\n",
    "    # conditional avgs\n",
    "    plt.plot(ech_Du, w_given_Du.get(), 'k-', ms=0.1, label=xplot)\n",
    "    plt.plot(Du_given_w.get(), ech_w, 'b-', ms=0.1, label=yplot)\n",
    "else:\n",
    "    # joint pdf\n",
    "    fp = plt.pcolormesh(X, Y, np.log10(Du_w))\n",
    "    # conditional avgs\n",
    "    plt.plot(ech_Du, w_given_Du, 'k-', ms=0.1, label=xplot)\n",
    "    plt.plot(Du_given_w, ech_w, 'b-', ms=0.1, label=yplot)\n",
    "\n",
    "# plotting params\n",
    "plt.xlabel(r'$\\frac{\\mathscr{D}^u_{\\ell=26.5\\eta}}{\\epsilon}(t=0)$')\n",
    "plt.ylabel(r'$\\frac{\\omega}{\\langle\\omega\\rangle}(t=0)$')\n",
    "plt.set_cmap('gist_ncar')\n",
    "plt.colorbar(fp)\n",
    "plt.legend(loc=0)\n",
    "plt.xlim(-10, 15)\n",
    "plt.ylim(0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e002d606",
   "metadata": {},
   "source": [
    "### 2.1.10. Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d353c0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_Du_w = pp.correlations(Du_w, bins=bins, ranges=ranges, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26eb3765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "if gpu:\n",
    "    fp = plt.pcolormesh(X, Y, corr_Du_w.get())\n",
    "else:\n",
    "    fp = plt.pcolormesh(X, Y, corr_Du_w)\n",
    "    \n",
    "# plotting params\n",
    "plt.xlabel(r'$\\frac{\\mathscr{D}^u_{\\ell=26.5\\eta}}{\\epsilon}(t=0)$')\n",
    "plt.ylabel(r'$\\frac{\\omega}{\\langle\\omega\\rangle}(t=0)$')\n",
    "plt.set_cmap('gist_ncar')\n",
    "plt.colorbar(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f851c68",
   "metadata": {},
   "source": [
    "For visualization improvements, please refer to section 2.1.12."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f08b024",
   "metadata": {},
   "source": [
    "### 2.1.11. Statistics sub-spaces \n",
    "At initialization, we used the option <code>stat='full'</code>. This enables the post-processing to take ALL points into account. We can specify other type of post-processing.\n",
    "\n",
    "#### 2.1.11.1.  Bulk/Interior\n",
    "<code>stat='bulk'</code> selects all points with $r\\leq 0.1$ and $|z|\\leq 0.1$. <br>\n",
    "<code>stat='interior'</code> selects all points with $|z|\\leq 0.69$.<br>\n",
    "If you want to change the type after initialization you can just do as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be79c75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.set_stats('bulk')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991b577a",
   "metadata": {},
   "source": [
    "All future calculations will now only take into account relevant points. You don't need to specify anything else (all the 2.1. section remains valid)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0325871a",
   "metadata": {},
   "source": [
    "#### 2.1.11.2. Penal\n",
    "<code>stat='penal'</code> modifies SFEMaNS penalization field to be a real indicator function. $\\texttt{penal}$ returns 1 if $\\texttt{penal} > 0.8$ and 0 otherwise. When activating the penalized postprocessing, because it is close to impossible to effectively load a full field, you always need to specify a slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da7d3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.set_stats('penal')\n",
    "pp.penal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47c2302",
   "metadata": {},
   "source": [
    "As you can see, penal is a full delayed array. If you want to launch a calc, the code will raise an error and tell you to provide a slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc28494",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.mean(Du_0, type='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e776c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = slice(None)\n",
    "pp.set_penal(s, [0], s, s)\n",
    "# s1, s2, s3, s4 are the slices to make on D, T, P, N axis\n",
    "pp.penal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4113a24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.mean(Du_0, type='both')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a052aa8e",
   "metadata": {},
   "source": [
    "Once the penalization is effectively set, all the 2.1 section remains valid (you don't need to specify anything else)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d98364",
   "metadata": {},
   "source": [
    "### 2.1.12. Visualization tips\n",
    "In case you want to use the blue-to-red rainbow colormap used in Faller *et al.*, since it is not natively implemented in matplotlib; use instead the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008b5281",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "N = 256\n",
    "colors = [(0,0,1,0.75), (0,0,0.5), (0,1,1), (0,0.5,0), (1,1,0), (1,0.5,0), (0.5,0,0), (1,0,0,0.75)]\n",
    "cmap_name = 'btr_rainbow'\n",
    "cmap = LinearSegmentedColormap.from_list(cmap_name, colors, N=N)\n",
    "plt.register_cmap(cmap=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0e07b1",
   "metadata": {},
   "source": [
    "Finally, vanilla correlations colorbars are most of times useless: the correlations' scope is so large that rare events (rare in the sense that very few bins captured them) skew the colorbar's gradient (look at figure in section 2.1.10 for example). When post-processing on 1 target, it won't be a problem because you can manually set the <code>vmin/vmax</code> arguments after visualizing a plot. When working with 20+ targets, it will consume quite some time to check each plot and manually fix the colorbar limit. Moreover, it is better if all the figures return the same 0-correlation. <br>\n",
    "As a consequence, I wrote a code that automatically finds convenient colorbar limits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa21189f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import TwoSlopeNorm\n",
    "\n",
    "# Finding visible colorbar range\n",
    "if gpu:\n",
    "    corr = corr_Du_w.get()\n",
    "corr[corr == -np.inf] = np.nan\n",
    "\n",
    "# corr is histogrammed to check the number of pixels per interval\n",
    "n_bins, v = np.histogram(corr[~np.isnan(corr)], bins=100)\n",
    "cutoff = bins\n",
    "# we only select the regions having more pixels that a cutoff number\n",
    "vmin = v[:-1][n_bins>cutoff][0]\n",
    "vmax = v[1:][::-1][(n_bins>cutoff)[::-1]][0]\n",
    "print('vmax = %f' % vmax)\n",
    "print('vmin = %f' % vmin)\n",
    "\n",
    "divnorm = TwoSlopeNorm(vmin=vmin, vcenter=0, vmax=vmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeb541e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "fp = plt.pcolormesh(X, Y, corr, cmap='gist_ncar', norm=divnorm)\n",
    "    \n",
    "# plotting params\n",
    "plt.xlabel(r'$\\frac{\\mathscr{D}^u_{\\ell=26.5\\eta}}{\\epsilon}(t=0)$')\n",
    "plt.ylabel(r'$\\frac{\\omega}{\\langle\\omega\\rangle}(t=0)$')\n",
    "\n",
    "plt.colorbar(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bb841b",
   "metadata": {},
   "source": [
    "## 2.2. Real Post-Processing (full-fields)\n",
    "\n",
    "Now that you know basic usage of the module, you can guess yourself that it won't really apply to *real* post-processing where we rather deal with fields much bigger in size. In this section, we will not really explain the principles of the memory schedulers (mainly *Divide-and-conquer algorithms*); we will instead give some case studies relevant to my internship. Because the computations are quite long (typically > 1h), we will not provide any executable code-block in the present section. Instead, we will reference in the **<code>examples</code>** directory a post-processing workflow to calculate Full-$\\left(\\mathscr{D}^u,\\omega\\right)$ statistics for 4 volume sub-spaces and two filtering scales.\n",
    "\n",
    "### 2.2.1. Parameters calculations\n",
    "As I explained in my report, prior to a \"real\" workflow, you must compute some global parameters which are inputs of the stats-calcs. The <code>misc</code> directory yields scripts enabling the user to get those. More precisely, the <code>externals.py</code> script computes the ranges and averages for $\\mathscr{D}^u$ and $\\omega$ for each type of statistic (bulk, interior, penal and full) by restructuring the data (sequentially on a CPU or GPU depending on the allocated ressources). <ins>Every statistic sub-space/field is launched in parallel by the use of job arrays</ins>. Follow the next steps in order to get the runs parameters:\n",
    "\n",
    "1. Build the post-processing tree:\n",
    "    ```console\n",
    "    sh make_tree.sh\n",
    "    ```\n",
    "    This bash script makes the following tree-structure:\n",
    "   ```\n",
    "    Du\n",
    "    │   *.py\n",
    "    │   *.sh\n",
    "    └───figs\n",
    "    │   └───logs\n",
    "    │   └───Du_l1\n",
    "    │   └───Du_2\n",
    "    │\n",
    "    └───misc\n",
    "    │   └───logs\n",
    "    │   └───*.py\n",
    "    │   └───*.sh\n",
    "    │\n",
    "    └───Du_l1\n",
    "    │   └───bulk\n",
    "    │   └───interior\n",
    "    │   └───penal\n",
    "    │   └───full\n",
    "    │\n",
    "    └───Du_2\n",
    "         ⋮\n",
    "    ```\n",
    "1. Launch all the jobs in parallel with a cpu or gpu job (you must choose in between `externals_cpu/gpu.sh`)\n",
    "    ```console\n",
    "    cd misc\n",
    "    sbatch externals_XXX.sh\n",
    "    ```\n",
    "\n",
    "1. The averages and ranges for each statistic and each field are now saved in `misc/miscs.csv`. They are accessible with `pandas`:\n",
    "    ```\n",
    "    import pandas as pd\n",
    "    import ast\n",
    "    \n",
    "    stat = '____'\n",
    "    field = '____'\n",
    "    params = pd.read_csv('misc/miscs.csv')\n",
    "    params.set_index('Stat', inplace=True)\n",
    "    f_params = ast.literal_eval(params.loc[stat][field])\n",
    "    f_avg = field_params[0]\n",
    "    f_range = field_params[1]\n",
    "    ```\n",
    "They will be used in next sections' workflows.\n",
    "\n",
    "### 2.2.2. Full-$\\left(\\mathscr{D}^u,\\omega\\right)$ joint PDFs computations\n",
    "Just like in the previous section, we wrote a general array-like script <code>joint_pdf.py</code> computing the joint PDF for $\\mathscr{D}^u$ and $\\omega$ for each type of statistic (bulk, interior, penal and full) and each scale by restructuring the data. If you want to execute the computations, you only have to launch 1 job (and to choose the ressources):\n",
    "```console\n",
    "sbatch joint_pdf_XXX.sh\n",
    "```\n",
    "At this point, all the partial joint pdfs (based on the data-restructuring) are written in binary `.dat` files (in the relevent field/stat dirs). The final recombination will take place right before the final processing. <ins>Keeping the `.dat` files</ins> can be useful to analyze statistics between decorrelated sets (if the restructuring was based on the temporal axis) but it actually is **mandatory to not redo the calculations if considering multiple visualizations**.\n",
    "\n",
    "### 2.2.3. Full-$\\left(\\mathscr{D}^u,\\omega\\right)$ statistics plotting\n",
    "In the `load_pdf.py` script, the partial joint pdfs are ultimately added to build the total density function. Just as before, a single job will finally plot every single figure (joint/corr for each stat and each scale) in the `figs` directory. Only a CPU job is given here because plotting only requires basic operations (file reading + matplotlib). Just execute:\n",
    "```console\n",
    "sbatch load_pdf.sh\n",
    "```\n",
    "### 2.2.4. Generalizations\n",
    "\n",
    "In case you want to deploy specific workloads, I give you some ideas on how you can modify the provided scripts:\n",
    "- make sure to efficiently restructure the data for memory management. <ins>Adapt the workloads' size after reading sections 3.2, C.2.1 and D from my report</ins>.\n",
    "- change the <code>make_tree.sh</code> file to be specific to your fields/types of statistics. In case you want to deploy +20 workloads, you might want to make the tree directly in <code>joint_pdf.py</code> (use for example <code>itertools</code> and `os.mkdir` to build a tree with all possible pairs of fields existing in the flow...)\n",
    "- the <code>externals.py</code> script will work regardless of the nature of the field (3D or 1D)\n",
    "- for the plotting, you should first plot *vanilla* graphs and only then rescale them properly (x/ylim). Finally incorporate them in your general script to be compatible with job arrays."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
